{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.envs/huggingface/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/student/nlp_da_comparison/models/load_model.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000000vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000000vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer,AutoModelForSequenceClassification\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000000vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000000vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000000vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998247027397156}, {'label': 'POSITIVE', 'score': 0.9996428489685059}, {'label': 'NEGATIVE', 'score': 0.9970999956130981}, {'label': 'NEGATIVE', 'score': 0.9918795228004456}, {'label': 'POSITIVE', 'score': 0.9986391663551331}]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model_name2 = 'autoevaluate/multi-class-classification'\n",
    "\n",
    "classifirer = pipeline('text-classification', model=model_name)\n",
    "results = classifirer([\n",
    "    'Price, Good Picture Quality, Ease of use.',\n",
    "    'compact size',\n",
    "    'Cheap and great resolution for digital photography and graphics',\n",
    "    \"can't download ringtones\",\n",
    "    'Good. Light, affordable, just like the C80.'\n",
    "    \"Didn't we just buy 12 AA batteries last week????\"]\n",
    "    )\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split into train and test\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_csv('../data/train/cr/train.txt', sep='|', header=None, names=['text'])\n",
    "df_test = pd.read_csv('../data/test/cr/test.txt', sep='|', header=None, names=['text'])\n",
    "df_train['class'] = df_train['text'].apply(lambda x: x.split('\\t')[0])\n",
    "df_train['text'] = df_train['text'].apply(lambda x: x.split('\\t')[1])\n",
    "df_test['class'] = df_test['text'].apply(lambda x: x.split('\\t')[0])\n",
    "df_test['text'] = df_test['text'].apply(lambda x: x.split('\\t')[1])\n",
    "\n",
    "df_train = df_train[['class', 'text']]\n",
    "df_test = df_test[['class', 'text']]\n",
    "\n",
    "X_test = df_test['text'].values.tolist()\n",
    "y_test = df_test['class'].values.tolist()\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train['text'], df_train['class'], test_size=0.2, random_state=42)\n",
    "X_train = X_train.values.tolist()\n",
    "y_train = y_train.values.tolist()\n",
    "X_val = X_val.values.tolist()\n",
    "y_val = y_val.values.tolist()\n",
    "\n",
    "df_val = pd.DataFrame({'class': y_val, 'text': X_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, return_tensors='tf')\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, return_tensors='tf')\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ = datasets.Dataset.from_dict(df_train)\n",
    "df_test_ = datasets.Dataset.from_dict(df_test)\n",
    "#df_val_ = datasets.Dataset.from_dict(df_val)\n",
    "dataset_final = datasets.DatasetDict({'train': df_train_, 'test': df_test_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afe6c13669f4bb2812a4816a21aa539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6497761ce7c9414c9b8d4eadf12658e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset_final.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_datasets[\"train\"]\n",
    "tokenized_test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DefaultDataCollator' from 'transformers' (/home/student/.envs/da_project/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/student/nlp_da_comparison/models/load_model.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DefaultDataCollator\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/student/nlp_da_comparison/models/load_model.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m data_collator \u001b[39m=\u001b[39m DefaultDataCollator(return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DefaultDataCollator' from 'transformers' (/home/student/.envs/da_project/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

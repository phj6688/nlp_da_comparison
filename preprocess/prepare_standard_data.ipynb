{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/'\n",
    "#test_folder = data_folder + 'test/'\n",
    "train_folder = data_folder + 'train/'\n",
    "list_train_paths = [join(train_folder,f+'/train.txt') for f in listdir(train_folder)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/train/cr/train.txt',\n",
       " '../data/train/cardio/train.txt',\n",
       " '../data/train/pc/train.txt',\n",
       " '../data/train/subj/train.txt',\n",
       " '../data/train/sst2/train.txt',\n",
       " '../data/train/kaggle_med/train.txt',\n",
       " '../data/train/trec/train.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_format_correction(path):\n",
    "    with open(path,'r') as f:    \n",
    "        lines = f.readlines()    \n",
    "        for line in lines:\n",
    "            if '\\t' not in line:\n",
    "                try:\n",
    "                    line.replace(' ', '\\t', 1)\n",
    "\n",
    "                except:\n",
    "                    print('error')\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads data from a txt file.\n",
    "    \"\"\"\n",
    "    # check file format\n",
    "    if path.endswith('.txt'):\n",
    "        df = pd.read_csv(path, sep='|', header=None, names=['text'])\n",
    "        try:\n",
    "            df['class'] = df['text'].apply(lambda x: x.split('\\t')[0])\n",
    "            df['text'] = df['text'].apply(lambda x: x.split('\\t')[1])\n",
    "        except:\n",
    "            df['class'] = df['text'].apply(lambda x: x.split(' ',1)[0])\n",
    "            df['text'] = df['text'].apply(lambda x: x.split(' ',1)[1])\n",
    "\n",
    "        df = df[['class', 'text']]\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError('File format not supported.')\n",
    "\n",
    "\n",
    "def generate_sample(path,sample_size):    \n",
    "    if check_format_correction(path):\n",
    "        output_path = path.replace('.txt','1000_sample.txt')\n",
    "        df = load_data(path)\n",
    "        number_of_classes = df['class'].nunique()\n",
    "        lowest_class_count = df['class'].value_counts().min()\n",
    "        samples_per_class = int(sample_size/number_of_classes)\n",
    "        if samples_per_class > lowest_class_count:\n",
    "            samples_per_class = lowest_class_count\n",
    "        new_df = df.groupby('class').apply(lambda x: x.sample(samples_per_class))\n",
    "        new_df = new_df.sample(frac=1).reset_index(drop=True)   \n",
    "        np.savetxt(output_path, new_df.values,fmt='%s',delimiter='\\t')\n",
    "        sample_name = output_path.split('/')[-2]\n",
    "        print(f'sample {sample_name} with {len(new_df)} sentences is saved to: ',output_path)\n",
    "    else:\n",
    "        print('check the format of the file: ',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample cr with 1000 sentences is saved to:  ../data/train/cr/train1000_sample.txt\n",
      "sample pc with 1000 sentences is saved to:  ../data/train/pc/train1000_sample.txt\n",
      "sample subj with 1000 sentences is saved to:  ../data/train/subj/train1000_sample.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_train_paths = ['../data/train/cr/train.txt',\n",
    " '../data/train/pc/train.txt',\n",
    " '../data/train/subj/train.txt',\n",
    " ]\n",
    "sample_size = 1000                 \n",
    "for path in tqdm(list_train_paths):\n",
    "    generate_sample(path,sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../eda_code/txt_for_test/train/sst2/train1000_sample.txt',\n",
       " '../eda_code/txt_for_test/train/trec/train1000_sample.txt',\n",
       " '../eda_code/txt_for_test/train/cr/train1000_sample.txt',\n",
       " '../eda_code/txt_for_test/train/cardio/train1000_sample.txt',\n",
       " '../eda_code/txt_for_test/train/subj/train1000_sample.txt',\n",
       " '../eda_code/txt_for_test/train/pc/train1000_sample.txt',\n",
       " '../eda_code/txt_for_test/train/kaggle_med/train1000_sample.txt']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_folder = '../eda_code/txt_for_test/train/'\n",
    "list_train_paths = [join(train_folder,f+'/train1000_sample.txt') for f in listdir(train_folder)]\n",
    "list_train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_train_paths:\n",
    "    check_format_correction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('../data/train/cr/train1000_sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "class      \n",
       "0       500\n",
       "1       500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['class']).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare word2vec dictionary for 1000 sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../experiments/data/'\n",
    "list_train_paths = [join(data_folder,f+'/train1000_sample.txt') for f in listdir(data_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../experiments/data/cr/train1000_sample.txt',\n",
       " '../experiments/data/pc/train1000_sample.txt',\n",
       " '../experiments/data/subj/train1000_sample.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocab(file_path,glove_path):\n",
    "    vocab = set()\n",
    "    vocab_not_in_glove = set()\n",
    "    _glove = open(glove_path, 'r').readlines()\n",
    "    w2v = {}\n",
    "    df = load_data(file_path)\n",
    "    for i in df['text']:\n",
    "        for j in i.split(' '):\n",
    "            if j not in vocab:\n",
    "                vocab.add(j)\n",
    "    print(f'size of unique words in {file_path} : {len(vocab)}')\n",
    "    for i in _glove:\n",
    "        word = i.split(' ')[0]\n",
    "        if word in vocab:\n",
    "            vec = i.split(' ')[1:]\n",
    "            w2v[i.split(' ')[0]] = np.asarray(vec, dtype='float32')\n",
    "    for i in vocab:\n",
    "        if i not in w2v:\n",
    "            vocab_not_in_glove.add(i)\n",
    "            \n",
    "\n",
    "    print(f'matches between vocab and glove for {file_path}: {len(w2v)}')\n",
    "    print(f'words in vocab but not in glove for {file_path}: {len(vocab_not_in_glove)}')\n",
    "    \n",
    "    return w2v, vocab_not_in_glove\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = 'glove.840B.300d.txt'\n",
    "file_path = '../experiments/data/subj/train1000_sample.txt'\n",
    "pickel_path = '../experiments/data/subj/wor2vec_1000_sample.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of unique words in ../experiments/data/subj/train1000_sample.txt : 6013\n",
      "matches between vocab and glove for ../experiments/data/subj/train1000_sample.txt: 5547\n",
      "words in vocab but not in glove for ../experiments/data/subj/train1000_sample.txt: 466\n"
     ]
    }
   ],
   "source": [
    "new_w2v, not_in_vocab = gen_vocab(file_path,glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shyamalan's\n",
      "breen's\n",
      "'fun\n",
      "pouqussimos\n",
      "b-movie-and-proud-of-it\n",
      "mazzotta\n",
      "'grasp'\n",
      "'great\n",
      "'enough'\n",
      "hartdegen\n",
      "there's\n",
      "self-narrated\n",
      "petser&#180\n",
      "half-sister's\n",
      "chill'\n",
      "hope'\n",
      "risotadas\n",
      "maslakh\n",
      "1970's\n",
      "danny's\n",
      "mouglalis\n",
      "'my\n",
      "sarah's\n",
      "hobbit'\n",
      "prozium\n",
      "women's\n",
      "brosnan's\n",
      "mackendrick\n",
      "renner's\n",
      "jesus'\n",
      "bale's\n",
      "version's\n",
      "lemle\n",
      "guzmn\n",
      "disney's\n",
      "wedding]\n",
      "guys'\n",
      "shouldn't\n",
      "stuart's\n",
      "[griffith]\n",
      "talkiness\n",
      "[dong]\n",
      "geonosis\n",
      "roommate's\n",
      "shreve's\n",
      "unconned\n",
      "script's\n",
      "girlfriend's\n",
      "boss's\n",
      "invadir&#237\n",
      "naipaul\n",
      "payami\n",
      "haven't\n",
      "tornatore's\n",
      "year's\n",
      "busc&#243\n",
      "salenger\n",
      "bielinsky's\n",
      "$20\n",
      "superlarge\n",
      "snowball's\n",
      "enfrentarse\n",
      "bogdanich\n",
      "makhmalbaf\n",
      "you've\n",
      "resources]\n",
      "feature-\n",
      "alicia's\n",
      "pulpiness\n",
      "walter's\n",
      "'starchildren'\n",
      "fererra's\n",
      "zimmett\n",
      "necessary-manipulation\n",
      "budcasso\n",
      "they've\n",
      "kasner\n",
      "pair's\n",
      "po'boys\n",
      "world's\n",
      "serry\n",
      "it'll\n",
      "&#193\n",
      "'safe\n",
      "'monsters'\n",
      "bettien\n",
      "#626\n",
      "perrini\n",
      "[a]\n",
      "rock'\n",
      "otte's\n",
      "'difficult'\n",
      "'been\n",
      "tomaselli\n",
      "harris's\n",
      "natalio\n",
      "waiting'\n",
      "tornatore\n",
      "couldn't\n",
      "sprechers\n",
      "dean's\n",
      "baumel\n",
      "wells'\n",
      "ana's\n",
      "woman's\n",
      "margr&#233\n",
      "won't\n",
      "andr&#225\n",
      "flash/tony\n",
      "scherfig\n",
      "kid's]\n",
      "that'\n",
      "she'll\n",
      "cho's\n",
      "admission's\n",
      "france's\n",
      "jack's\n",
      "spirit-fighting\n",
      "panderetas\n",
      "tommy's\n",
      "games'\n",
      "youht\n",
      "100%\n",
      "'drumline'\n",
      "spielrein\n",
      "gore-free\n",
      "[franois\n",
      "ningum\n",
      "fianc&#233\n",
      "bruckheimeresque\n",
      "[is]\n",
      "obi-wan's\n",
      "out-bad-act\n",
      "chevrah\n",
      "hitchcockian\n",
      "'gammasphere'\n",
      "wolodarsky\n",
      "$25\n",
      "marchelletta\n",
      "journey'\n",
      "vilhj&#225\n",
      "chabrol's\n",
      "verbenesca\n",
      "necklages\n",
      "pigeonhole-resisting\n",
      "'lovely\n",
      "speeds/\n",
      "isn't\n",
      "'plain'\n",
      "kiarostami\n",
      "springs'\n",
      "moretti's\n",
      "generation's\n",
      "they're\n",
      "quinn's\n",
      "lear's\n",
      "narva&#180\n",
      "ej&#233\n",
      "neighbor's\n",
      "burns'\n",
      "lynch-ian\n",
      "gaitskill's\n",
      "'comedy'\n",
      "operation&#237\n",
      "[time\n",
      "siegel's\n",
      "hasn't\n",
      "dawn's\n",
      "defino\n",
      "'christian\n",
      "cantet\n",
      "miyazaki's\n",
      "yavakri\n",
      "cia's\n",
      "roteirista\n",
      "edgar's\n",
      "verveen\n",
      "sang-woo\n",
      "father's\n",
      "cruise's\n",
      "murbah\n",
      "minister's\n",
      "'how\n",
      "'while\n",
      "wasn't\n",
      "'fatal\n",
      "silmarillion'\n",
      "grunge-pirate\n",
      "adams'\n",
      "splat-man\n",
      "'scratch'\n",
      "ten&#237\n",
      "vijayendra\n",
      "'murder\n",
      "lee's\n",
      "kwan's\n",
      "[kidd]\n",
      "rohmer\n",
      "csupo\n",
      "actors'\n",
      "sruggling\n",
      "sha's\n",
      "sharonna\n",
      "parker's\n",
      "[monsoon\n",
      "movies'\n",
      "robert's\n",
      "teenage-son\n",
      "ventura's\n",
      "family's\n",
      "lewis'\n",
      "vintage-tv\n",
      "afghan-pakistan\n",
      "carpenter's\n",
      "consigliori\n",
      "[t]he\n",
      "thought-to-be-dead\n",
      "nayomi\n",
      "film's\n",
      "projeo\n",
      "aren't\n",
      "'red\n",
      "duo's\n",
      "bob's\n",
      "animated-movie\n",
      "nephew/surrogate\n",
      "altman-ish\n",
      "hjejle\n",
      "too-facile\n",
      "[denis']\n",
      "we'll\n",
      "author's\n",
      "'nine\n",
      "ghagte\n",
      "'lick\n",
      "cor-blimey-luv-a-duck\n",
      "non-bondish\n",
      "blackmail-they\n",
      "rorion\n",
      "clung-to\n",
      "lzli\n",
      "prentis\n",
      "'bartleby'\n",
      "sanjuanera\n",
      "lord's\n",
      "heroine's\n",
      "china's\n",
      "kreskin\n",
      "hadley's\n",
      "rcito\n",
      "seinfeld's\n",
      "balsiger\n",
      "ploddy\n",
      "natural-seeming\n",
      "roll's\n",
      "lizt\n",
      "thriller's\n",
      "grandmother's\n",
      "else's\n",
      "mcconaughey's\n",
      "kumin\n",
      "grav&#237\n",
      "smart-mouthed-but-became-polite\n",
      "and-doughnut\n",
      "fantazises\n",
      "irwin's\n",
      "derisions\n",
      "'refreshing\n",
      "jen's\n",
      "larry's\n",
      "eisenberg's\n",
      "really-smart\n",
      "anyone's\n",
      "tykwer's\n",
      "mepe's\n",
      "finnur\n",
      "aksel\n",
      "1700's\n",
      "sequer\n",
      "cornuke\n",
      "you'd\n",
      "standiford\n",
      "he'll\n",
      "michle's]\n",
      "podbielska\n",
      "hanna-barbera\n",
      "nick's\n",
      "daddy's\n",
      "trasha\n",
      "paro's\n",
      "vapoorizer\n",
      "lena's\n",
      "scott's\n",
      "yakavetta\n",
      "russian-jewish\n",
      "covardia\n",
      "labute's\n",
      "edgy--merely\n",
      "redemption-spend\n",
      "odete\n",
      "weren't\n",
      "[but]\n",
      "polanski's\n",
      "paccard\n",
      "'big\n",
      "tuba-playing\n",
      "hatosy\n",
      "michael's\n",
      "grace's\n",
      "so-five-minutes-ago\n",
      "'artstico'\n",
      "yoba\n",
      "writer's\n",
      "bickle'\n",
      "tom's\n",
      "freundlich's\n",
      "imagination'\n",
      "sannah\n",
      "j&#243\n",
      "jolgorio\n",
      "hart's\n",
      "numerobis\n",
      "majidi's\n",
      "oklahoma's\n",
      "ronwell\n",
      "'the\n",
      "verde&#187\n",
      "camp's\n",
      "2--quite\n",
      "they'd\n",
      "liebman's\n",
      "oscar-winner\n",
      "charles'\n",
      "'what\n",
      "pullitzer\n",
      "trmino\n",
      "children's\n",
      "claire's\n",
      "brumder\n",
      "dealer's\n",
      "ex-\n",
      "ship's\n",
      "python's\n",
      "gender-war\n",
      "film-culture\n",
      "eigenberg\n",
      "video-game-based\n",
      "ovitz\n",
      "melina's\n",
      "caffeine-\n",
      "mitchener\n",
      "&#38\n",
      "iran's\n",
      "island's\n",
      "woo's\n",
      "food-spittingly\n",
      "yun-fat\n",
      "roxie's\n",
      "post-9/11\n",
      "simbolizando\n",
      "unsurface\n",
      "attal\n",
      "who's\n",
      "soul-stripping\n",
      "collinwood\n",
      "ride's\n",
      "i'm\n",
      "dreamplace\n",
      "olsen's\n",
      "fracasso\n",
      "queen's\n",
      "girl's\n",
      "majidi\n",
      "rapsploitation\n",
      "ensemble-driven\n",
      "kids'\n",
      "[smith]\n",
      "[wang]\n",
      "&#171\n",
      "roger's\n",
      "d&#218\n",
      "hawk-style\n",
      "'extreme'\n",
      "disyuntiva\n",
      "shainberg's\n",
      "denis'\n",
      "holmes</a>\n",
      "1995's\n",
      "'dragonfly'\n",
      "moore's\n",
      "policicar\n",
      "burdette's\n",
      "fiancee's\n",
      "bard's\n",
      "subject's\n",
      "by^&#197\n",
      "shandling\n",
      "[the\n",
      "age-inspired\n",
      "mediocridade\n",
      "town's\n",
      "'deliverance'\n",
      "burgh&#246\n",
      "already-bursting\n",
      "maker's\n",
      "she's\n",
      "movie's\n",
      "plot's\n",
      "i'd\n",
      "police's\n",
      "shainberg\n",
      "what's\n",
      "pro-serbian\n",
      "keep-'em-guessing\n",
      "wouldn't\n",
      "emperor's\n",
      "under-bossman\n",
      "desplat's\n",
      "solondz\n",
      "actor's\n",
      "bruce's\n",
      "fabbrizio\n",
      "grant's\n",
      "we'd\n",
      "chabrol\n",
      "fix--this\n",
      "man's\n",
      "parents'\n",
      "everyone's\n",
      "colocaba\n",
      "1777-1791\n",
      "scorsese's\n",
      "steve's\n",
      "bachelorman's\n",
      "feardotcom\n",
      "love's\n",
      "we've\n",
      "rainone\n",
      "husband's\n",
      "'dimension\n",
      "lmsd&#243\n",
      "kieslowski's\n",
      "he's\n",
      "a&#239\n",
      "jae-eun\n",
      "responsvel\n",
      "it--and\n",
      "modern-office\n",
      "relationship-induced\n",
      "friend's\n",
      "nicholas's\n",
      "greene's\n",
      "overmanipulative\n",
      "years/\n",
      "filmmaker's\n",
      "numbers'\n",
      "'boys'\n",
      "slash-and-hack\n",
      "lasker's\n",
      "chouraqui\n",
      "klasky\n",
      "matt's\n",
      "rings'\n",
      "kline's\n",
      "mother's\n"
     ]
    }
   ],
   "source": [
    "for i in not_in_vocab:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_w2v, open(pickel_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3faeff2a81486748f40de2e4b0026ebe23fa603fbf9deacac79c36c26bb6d0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

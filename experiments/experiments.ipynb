{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 22:55:04.487433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 22:55:04.788119: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-06 22:55:05.430231: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 22:55:05.430279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 22:55:05.430284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from aug import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<input type='checkbox'> Train model on Original + Augmented data (with different methods) and test it </input> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "datasets = ['pc','cr','subj']\n",
    "aug_methods = ['eda_augmenter','wordnet_augmenter','aeda_augmenter','backtranslation_augmenter','clare_augmenter']\n",
    "n_samples = [1,2,4,8,10]\n",
    "#path = f'data/{dataset}/train_{dataset}_{aug_method}_n_samples_{n_sample}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        words = line.split()\n",
    "        words = words[:batch_size] #cut off if too long\n",
    "        for j, word in enumerate(words):\n",
    "            if word in w2v:\n",
    "                x_matrix[i, j, :] = w2v[word]\n",
    "\n",
    "    return x_matrix\n",
    "\n",
    "\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    # print(f'F1: {f1}')\n",
    "    # print(f'Precision: {precision}')\n",
    "    # print(f'Recall: {recall}')\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=1024,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    acc_aug, f1_aug, precision_aug, recall_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    acc_original, f1_original, precision_original, recall_original = evaluate_model(model_original, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {acc_original} \\n f1: {f1_original} \\n precision: {precision_original} \\n recall: {recall_original}')\n",
    "    print(f'augmented model: \\n acc: {acc_aug} \\n f1: {f1_aug} \\n precision: {precision_aug} \\n recall: {recall_aug}')\n",
    "    \n",
    "\n",
    "    # Save model\n",
    "    #model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run('cr','eda_augmenter',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('data/cr/train_cr_eda_augmenter_n_sample_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug, y_train_aug = df['text'].values, df['class'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train=to_categorical(y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from aug import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings\n",
    "\n",
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        words = line.split()\n",
    "        words = words[:batch_size] #cut off if too long\n",
    "        for j, word in enumerate(words):\n",
    "            if word in w2v:\n",
    "                x_matrix[i, j, :] = w2v[word]\n",
    "\n",
    "    return x_matrix\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))              \n",
    "    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'] )\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    #y_pred = to_categorical(y_pred.argmax(axis=1))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    # print(f'F1: {f1}')\n",
    "    # print(f'Precision: {precision}')\n",
    "    # print(f'Recall: {recall}')\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "    # y_train_aug = to_categorical(y_train_aug)\n",
    "    # y_train_original = to_categorical(y_train_original)\n",
    "    # y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    print('Creating matrices...')\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    print('Training model...')\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=1024,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    acc_aug, f1_aug, precision_aug, recall_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    acc_original, f1_original, precision_original, recall_original = evaluate_model(model_original, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {acc_original:.4f} \\n f1: {f1_original:.4f} \\n precision: {precision_original:.4f} \\n recall: {recall_original:.4f}')\n",
    "    print(f'augmented model: \\n acc: {acc_aug:.4f} \\n f1: {f1_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}')\n",
    "\n",
    "    print('finished')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     run('cr','eda_augmenter',4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name,aug_method,n_sample = 'cr','eda_augmenter',4\n",
    "\n",
    "print('Loading data...')\n",
    "path_train_original = f'data/{dataset_name}/train.txt'\n",
    "path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "path_test = f'data/{dataset_name}/test.txt'\n",
    "train_aug = load_data(path_train_aug)\n",
    "train_original = load_data(path_train_original)\n",
    "test_data = load_data(path_test)\n",
    "X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "# y_train_aug = to_categorical(y_train_aug)\n",
    "# y_train_original = to_categorical(y_train_original)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# load wor2vec pickle\n",
    "path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "# create matrices\n",
    "print('Creating matrices...')\n",
    "X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "# Train model\n",
    "print('Training model...')\n",
    "model_aug = build_model()\n",
    "model_original = build_model()\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                callbacks=callbacks, validation_split=0.1,\n",
    "                batch_size=1024,shuffle=True, verbose=0)\n",
    "model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                callbacks=callbacks, validation_split=0.1,\n",
    "                batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model...')\n",
    "\n",
    "# acc_aug, f1_aug, precision_aug, recall_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "# acc_original, f1_original, precision_original, recall_original = evaluate_model(model_original, X_test, y_test)\n",
    "# print(f'original model: \\n acc: {acc_original:.4f} \\n f1: {f1_original:.4f} \\n precision: {precision_original:.4f} \\n recall: {recall_original:.4f}')\n",
    "# print(f'augmented model: \\n acc: {acc_aug:.4f} \\n f1: {f1_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}')\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_original.predict(X_test)\n",
    "y = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(pred, y):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'HANDY AND : YOU CAN USE IT ALMOST ANYWHERE'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings\n",
    "\n",
    "\n",
    "\n",
    "def create_y_matrix(y_data):\n",
    "  y_matrix = np.zeros((len(y_data),2))\n",
    "  for count,i in enumerate(y_data):\n",
    "    if i == 1:\n",
    "      y_matrix[count][1] = 1.0\n",
    "    else:\n",
    "      y_matrix[count][0] = 1.0\n",
    "  return y_matrix\n",
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        # print(line)\n",
    "        # print(i)\n",
    "        try:\n",
    "            words = line.split()\n",
    "            words = words[:batch_size] #cut off if too long\n",
    "            for j, word in enumerate(words):\n",
    "                if word in w2v:\n",
    "                    x_matrix[i, j, :] = w2v[word]\n",
    "        except:\n",
    "            pass\n",
    "    return x_matrix\n",
    "\n",
    "\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))              \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',Precision(),Recall(),AUC()] )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    eval = model.evaluate(X_test, y_test)\n",
    "    loss = eval[0]\n",
    "    accuracy = eval[1]\n",
    "    precision = eval[2]\n",
    "    recall = eval[3]\n",
    "    auc = eval[4]    \n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    # print(\"Loss: \", loss)\n",
    "    # print(\"Accuracy: \", accuracy)\n",
    "    # print(\"Precision: \", precision)\n",
    "    # print(\"Recall: \", recall)\n",
    "    # print(\"AUC: \", auc)\n",
    "    # print ('f1 score: ', f1_score)\n",
    "    return loss, accuracy, precision, recall, auc, f1_score\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "    # y_train_aug = to_categorical(y_train_aug)\n",
    "    # y_train_original = to_categorical(y_train_original)\n",
    "    # y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    print('Creating matrices...')\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    print('Training model...')\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=1024,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    loss_org, accuracy_org, precision_org, recall_org, auc_org, f1_score_org = evaluate_model(model_original, X_test, y_test)\n",
    "    loss_aug, accuracy_aug, precision_aug, recall_aug, auc_aug, f1_score_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {accuracy_org:.4f} \\n f1: {f1_score_org:.4f} \\n precision: {precision_org:.4f} \\n recall: {recall_org:.4f}, \\n auc: {auc_org:.4f}')\n",
    "    print(f'augmented model: \\n acc: {accuracy_aug:.4f} \\n f1: {f1_score_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}, \\n auc: {auc_aug:.4f}')\n",
    "\n",
    "    print('finished')\n",
    "\n",
    "\n",
    "\n",
    "run('pc','aeda_augmenter',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import runai.ga.keras\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings\n",
    "\n",
    "\n",
    "\n",
    "def create_y_matrix(y_data):\n",
    "  y_matrix = np.zeros((len(y_data),2))\n",
    "  for count,i in enumerate(y_data):\n",
    "    if i == 1:\n",
    "      y_matrix[count][1] = 1.0\n",
    "    else:\n",
    "      y_matrix[count][0] = 1.0\n",
    "  return y_matrix\n",
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        try:\n",
    "            words = line.split()\n",
    "            words = words[:batch_size] #cut off if too long\n",
    "            for j, word in enumerate(words):\n",
    "                if word in w2v:\n",
    "                    x_matrix[i, j, :] = w2v[word]\n",
    "        except:\n",
    "            pass\n",
    "    return x_matrix\n",
    "\n",
    "\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))  \n",
    "\n",
    "    STEPS = 10000\n",
    "    optimizer = runai.ga.keras.optimizers.Optimizer(Adam(), steps=STEPS)            \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc',Precision(),Recall(),AUC()] )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    eval = model.evaluate(X_test, y_test)\n",
    "    loss = eval[0]\n",
    "    accuracy = eval[1]\n",
    "    precision = eval[2]\n",
    "    recall = eval[3]\n",
    "    auc = eval[4]    \n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    # print(\"Loss: \", loss)\n",
    "    # print(\"Accuracy: \", accuracy)\n",
    "    # print(\"Precision: \", precision)\n",
    "    # print(\"Recall: \", recall)\n",
    "    # print(\"AUC: \", auc)\n",
    "    # print ('f1 score: ', f1_score)\n",
    "    return loss, accuracy, precision, recall, auc, f1_score\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "    # y_train_aug = to_categorical(y_train_aug)\n",
    "    # y_train_original = to_categorical(y_train_original)\n",
    "    # y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    print('Creating matrices...')\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    print('Training model...')\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=128,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=128,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    loss_org, accuracy_org, precision_org, recall_org, auc_org, f1_score_org = evaluate_model(model_original, X_test, y_test)\n",
    "    loss_aug, accuracy_aug, precision_aug, recall_aug, auc_aug, f1_score_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {accuracy_org:.4f} \\n f1: {f1_score_org:.4f} \\n precision: {precision_org:.4f} \\n recall: {recall_org:.4f}, \\n auc: {auc_org:.4f}')\n",
    "    print(f'augmented model: \\n acc: {accuracy_aug:.4f} \\n f1: {f1_score_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}, \\n auc: {auc_aug:.4f}')\n",
    "\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run('pc','aeda_augmenter',8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024//100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aeda_large = pd.read_csv('results/pc_aeda_compare_with_large_w2v.csv', index_col=0)\n",
    "pc_aeda_small = pd.read_csv('results/pc_aeda_compare_with_small_w2v.csv', index_col=0)\n",
    "pc_eda_large = pd.read_csv('results/pc_eda_compare_with_large_w2v.csv', index_col=0)\n",
    "pc_eda_small = pd.read_csv('results/pc_eda_compare_with_small_w2v.csv', index_col=0)\n",
    "pc_wordnet_large = pd.read_csv('results/pc_wordnet_compare_with_large_w2v.csv', index_col=0)\n",
    "pc_wordnet_small = pd.read_csv('results/pc_wordnet_compare_with_small_w2v.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_results = [pc_aeda_large,pc_aeda_small,pc_eda_large ,pc_eda_small ,pc_wordnet_large,pc_wordnet_small]\n",
    "list_of_features = ['loss','accuracy','precision','recall','auc''f1_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(list_of_results)\n",
    "df = df.groupby(df.index,sort=False).mean()\n",
    "org_series = df['original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_results:\n",
    "    i['original'] = org_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_results:\n",
    "    i.to_csv(f'results/{i.index[0]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aeda_large.to_excel('results/pc_aeda_large_w2v.xlsx')\n",
    "pc_aeda_small.to_excel('results/pc_aeda_small_w2v.xlsx')\n",
    "pc_eda_large.to_excel('results/pc_eda_large_w2v.xlsx')\n",
    "pc_eda_small.to_excel('results/pc_eda_small_w2v.xlsx')\n",
    "pc_wordnet_large.to_excel('results/pc_wordnet_large_w2v.xlsx')\n",
    "pc_wordnet_small.to_excel('results/pc_wordnet_small_w2v.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_eda_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/pc_aeda_large_w2v.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/pc_aeda_small_w2v.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aeda_large = pd.read_csv('results/pc_aeda_large_w2v.csv', index_col=0)\n",
    "pc_aeda_small = pd.read_csv('results/pc_aeda_small_w2v.csv', index_col=0)\n",
    "pc_eda_large = pd.read_csv('results/pc_eda_large_w2v.csv', index_col=0)\n",
    "pc_eda_small = pd.read_csv('results/pc_eda_small_w2v.csv', index_col=0)\n",
    "pc_wordnet_large = pd.read_csv('results/pc_wordnet_large_w2v.csv', index_col=0)\n",
    "pc_wordnet_small = pd.read_csv('results/pc_wordnet_small_w2v.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_aeda_large.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_aeda_small.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_eda_large.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_eda_small.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_wordnet_large.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pc_wordnet_small.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "105d983fd7af724b4799aa016a3246070456ed2a0f153d4c18e29a0780e65f60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 22:19:21.524100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 22:19:21.673177: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-30 22:19:22.246518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 22:19:22.246570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-30 22:19:22.246576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from aug import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<input type='checkbox'> Train model on Original + Augmented data (with different methods) and test it </input> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "datasets = ['pc','cr','subj']\n",
    "aug_methods = ['eda_augmenter','wordnet_augmenter','aeda_augmenter','backtranslation_augmenter','clare_augmenter']\n",
    "n_samples = [1,2,4,8,10]\n",
    "#path = f'data/{dataset}/train_{dataset}_{aug_method}_n_samples_{n_sample}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        words = line.split()\n",
    "        words = words[:batch_size] #cut off if too long\n",
    "        for j, word in enumerate(words):\n",
    "            if word in w2v:\n",
    "                x_matrix[i, j, :] = w2v[word]\n",
    "\n",
    "    return x_matrix\n",
    "\n",
    "\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    # print(f'F1: {f1}')\n",
    "    # print(f'Precision: {precision}')\n",
    "    # print(f'Recall: {recall}')\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=1024,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    acc_aug, f1_aug, precision_aug, recall_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    acc_original, f1_original, precision_original, recall_original = evaluate_model(model_original, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {acc_original} \\n f1: {f1_original} \\n precision: {precision_original} \\n recall: {recall_original}')\n",
    "    print(f'augmented model: \\n acc: {acc_aug} \\n f1: {f1_aug} \\n precision: {precision_aug} \\n recall: {recall_aug}')\n",
    "    \n",
    "\n",
    "    # Save model\n",
    "    #model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 21:28:23.230280: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 548760000 exceeds 10% of free system memory.\n",
      "2022-11-30 21:28:30.618405: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60990000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "run('cr','eda_augmenter',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('data/cr/train_cr_eda_augmenter_n_sample_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug, y_train_aug = df['text'].values, df['class'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train=to_categorical(y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from aug import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings\n",
    "\n",
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        words = line.split()\n",
    "        words = words[:batch_size] #cut off if too long\n",
    "        for j, word in enumerate(words):\n",
    "            if word in w2v:\n",
    "                x_matrix[i, j, :] = w2v[word]\n",
    "\n",
    "    return x_matrix\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))              \n",
    "    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'] )\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    #y_pred = to_categorical(y_pred.argmax(axis=1))\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    # print(f'Accuracy: {acc}')\n",
    "    # print(f'F1: {f1}')\n",
    "    # print(f'Precision: {precision}')\n",
    "    # print(f'Recall: {recall}')\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "    # y_train_aug = to_categorical(y_train_aug)\n",
    "    # y_train_original = to_categorical(y_train_original)\n",
    "    # y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    print('Creating matrices...')\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    print('Training model...')\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=1024,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    acc_aug, f1_aug, precision_aug, recall_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    acc_original, f1_original, precision_original, recall_original = evaluate_model(model_original, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {acc_original:.4f} \\n f1: {f1_original:.4f} \\n precision: {precision_original:.4f} \\n recall: {recall_original:.4f}')\n",
    "    print(f'augmented model: \\n acc: {acc_aug:.4f} \\n f1: {f1_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}')\n",
    "\n",
    "    print('finished')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     run('cr','eda_augmenter',4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating matrices...\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 17:37:57.519181: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 548760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset_name,aug_method,n_sample = 'cr','eda_augmenter',4\n",
    "\n",
    "print('Loading data...')\n",
    "path_train_original = f'data/{dataset_name}/train.txt'\n",
    "path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "path_test = f'data/{dataset_name}/test.txt'\n",
    "train_aug = load_data(path_train_aug)\n",
    "train_original = load_data(path_train_original)\n",
    "test_data = load_data(path_test)\n",
    "X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "# y_train_aug = to_categorical(y_train_aug)\n",
    "# y_train_original = to_categorical(y_train_original)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# load wor2vec pickle\n",
    "path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "# create matrices\n",
    "print('Creating matrices...')\n",
    "X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "# Train model\n",
    "print('Training model...')\n",
    "model_aug = build_model()\n",
    "model_original = build_model()\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                callbacks=callbacks, validation_split=0.1,\n",
    "                batch_size=1024,shuffle=True, verbose=0)\n",
    "model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                callbacks=callbacks, validation_split=0.1,\n",
    "                batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model...')\n",
    "\n",
    "# acc_aug, f1_aug, precision_aug, recall_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "# acc_original, f1_original, precision_original, recall_original = evaluate_model(model_original, X_test, y_test)\n",
    "# print(f'original model: \\n acc: {acc_original:.4f} \\n f1: {f1_original:.4f} \\n precision: {precision_original:.4f} \\n recall: {recall_original:.4f}')\n",
    "# print(f'augmented model: \\n acc: {acc_aug:.4f} \\n f1: {f1_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}')\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model_original.predict(X_test)\n",
    "y = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16132694] [0. 1.]\n",
      "[0.21753316] [0. 1.]\n",
      "[0.1440181] [1. 0.]\n",
      "[0.988908] [0. 1.]\n",
      "[0.45082453] [1. 0.]\n",
      "[0.1314982] [1. 0.]\n",
      "[0.958217] [0. 1.]\n",
      "[0.14002636] [1. 0.]\n",
      "[0.79664737] [0. 1.]\n",
      "[0.17053236] [1. 0.]\n",
      "[0.14403117] [1. 0.]\n",
      "[0.984244] [0. 1.]\n",
      "[0.16144119] [1. 0.]\n",
      "[0.9854209] [0. 1.]\n",
      "[0.96171516] [0. 1.]\n",
      "[0.90594995] [0. 1.]\n",
      "[0.18187661] [0. 1.]\n",
      "[0.5948102] [0. 1.]\n",
      "[0.986675] [0. 1.]\n",
      "[0.15917762] [1. 0.]\n",
      "[0.149311] [1. 0.]\n",
      "[0.97655416] [0. 1.]\n",
      "[0.9887525] [0. 1.]\n",
      "[0.26659322] [0. 1.]\n",
      "[0.985523] [0. 1.]\n",
      "[0.9486603] [0. 1.]\n",
      "[0.13278478] [0. 1.]\n",
      "[0.9810031] [0. 1.]\n",
      "[0.600198] [1. 0.]\n",
      "[0.81589156] [0. 1.]\n",
      "[0.59023637] [1. 0.]\n",
      "[0.9427075] [0. 1.]\n",
      "[0.895047] [0. 1.]\n",
      "[0.9591308] [0. 1.]\n",
      "[0.15453428] [1. 0.]\n",
      "[0.18758851] [1. 0.]\n",
      "[0.68112564] [0. 1.]\n",
      "[0.24654442] [0. 1.]\n",
      "[0.22985588] [0. 1.]\n",
      "[0.12983397] [1. 0.]\n",
      "[0.6760976] [0. 1.]\n",
      "[0.7224203] [1. 0.]\n",
      "[0.60235476] [0. 1.]\n",
      "[0.98092353] [0. 1.]\n",
      "[0.9880489] [0. 1.]\n",
      "[0.98914605] [0. 1.]\n",
      "[0.25784612] [0. 1.]\n",
      "[0.9886365] [0. 1.]\n",
      "[0.97308433] [0. 1.]\n",
      "[0.9888789] [0. 1.]\n",
      "[0.13285127] [1. 0.]\n",
      "[0.9887457] [0. 1.]\n",
      "[0.1294084] [1. 0.]\n",
      "[0.9885628] [0. 1.]\n",
      "[0.8327754] [0. 1.]\n",
      "[0.74831474] [1. 0.]\n",
      "[0.13292675] [1. 0.]\n",
      "[0.988016] [0. 1.]\n",
      "[0.35271204] [1. 0.]\n",
      "[0.98056465] [0. 1.]\n",
      "[0.8458009] [0. 1.]\n",
      "[0.14366491] [1. 0.]\n",
      "[0.6357922] [0. 1.]\n",
      "[0.16680768] [1. 0.]\n",
      "[0.4631397] [1. 0.]\n",
      "[0.98679227] [0. 1.]\n",
      "[0.98105603] [0. 1.]\n",
      "[0.98730004] [0. 1.]\n",
      "[0.16745368] [1. 0.]\n",
      "[0.19708839] [0. 1.]\n",
      "[0.71416366] [1. 0.]\n",
      "[0.20285994] [0. 1.]\n",
      "[0.9894395] [0. 1.]\n",
      "[0.15626521] [1. 0.]\n",
      "[0.16801098] [1. 0.]\n",
      "[0.9782346] [0. 1.]\n",
      "[0.77820086] [0. 1.]\n",
      "[0.9883568] [0. 1.]\n",
      "[0.9275433] [0. 1.]\n",
      "[0.2984746] [1. 0.]\n",
      "[0.9861709] [0. 1.]\n",
      "[0.16615976] [1. 0.]\n",
      "[0.9891571] [0. 1.]\n",
      "[0.14563578] [0. 1.]\n",
      "[0.98735857] [0. 1.]\n",
      "[0.9850719] [0. 1.]\n",
      "[0.98907393] [0. 1.]\n",
      "[0.9871541] [0. 1.]\n",
      "[0.9848642] [0. 1.]\n",
      "[0.95283484] [0. 1.]\n",
      "[0.9830474] [0. 1.]\n",
      "[0.9240568] [0. 1.]\n",
      "[0.98072946] [1. 0.]\n",
      "[0.18646054] [0. 1.]\n",
      "[0.6069282] [1. 0.]\n",
      "[0.55816483] [0. 1.]\n",
      "[0.17155027] [1. 0.]\n",
      "[0.9310841] [0. 1.]\n",
      "[0.25210312] [0. 1.]\n",
      "[0.985874] [0. 1.]\n",
      "[0.97183293] [1. 0.]\n",
      "[0.8372472] [0. 1.]\n",
      "[0.9699083] [0. 1.]\n",
      "[0.18366699] [1. 0.]\n",
      "[0.98853755] [0. 1.]\n",
      "[0.14647971] [1. 0.]\n",
      "[0.14832987] [1. 0.]\n",
      "[0.9100314] [0. 1.]\n",
      "[0.13538992] [0. 1.]\n",
      "[0.45355883] [0. 1.]\n",
      "[0.9888622] [0. 1.]\n",
      "[0.9883473] [0. 1.]\n",
      "[0.24527961] [1. 0.]\n",
      "[0.9321368] [1. 0.]\n",
      "[0.9887268] [0. 1.]\n",
      "[0.95699525] [0. 1.]\n",
      "[0.14067586] [0. 1.]\n",
      "[0.17800024] [0. 1.]\n",
      "[0.9665666] [0. 1.]\n",
      "[0.14690018] [1. 0.]\n",
      "[0.98244375] [0. 1.]\n",
      "[0.20993747] [1. 0.]\n",
      "[0.81589156] [0. 1.]\n",
      "[0.8189307] [0. 1.]\n",
      "[0.98818004] [0. 1.]\n",
      "[0.859122] [0. 1.]\n",
      "[0.86453414] [0. 1.]\n",
      "[0.1301541] [1. 0.]\n",
      "[0.98565036] [0. 1.]\n",
      "[0.17197707] [0. 1.]\n",
      "[0.9641373] [0. 1.]\n",
      "[0.98758477] [0. 1.]\n",
      "[0.98414963] [0. 1.]\n",
      "[0.9894772] [0. 1.]\n",
      "[0.14509465] [1. 0.]\n",
      "[0.9422151] [0. 1.]\n",
      "[0.2678534] [1. 0.]\n",
      "[0.12497561] [1. 0.]\n",
      "[0.9852249] [0. 1.]\n",
      "[0.13709357] [1. 0.]\n",
      "[0.14288183] [1. 0.]\n",
      "[0.6392142] [0. 1.]\n",
      "[0.9868627] [0. 1.]\n",
      "[0.17339888] [0. 1.]\n",
      "[0.626027] [0. 1.]\n",
      "[0.93398815] [1. 0.]\n",
      "[0.9887803] [0. 1.]\n",
      "[0.43793836] [0. 1.]\n",
      "[0.98829114] [0. 1.]\n",
      "[0.98122996] [0. 1.]\n",
      "[0.9051851] [0. 1.]\n",
      "[0.7136901] [0. 1.]\n",
      "[0.13965103] [1. 0.]\n",
      "[0.94334567] [0. 1.]\n",
      "[0.22180708] [1. 0.]\n",
      "[0.17618927] [0. 1.]\n",
      "[0.8486463] [0. 1.]\n",
      "[0.9846759] [0. 1.]\n",
      "[0.20328423] [0. 1.]\n",
      "[0.9699045] [0. 1.]\n",
      "[0.9798235] [0. 1.]\n",
      "[0.16986507] [1. 0.]\n",
      "[0.14967458] [1. 0.]\n",
      "[0.18212786] [1. 0.]\n",
      "[0.48914495] [1. 0.]\n",
      "[0.98664826] [0. 1.]\n",
      "[0.91864353] [0. 1.]\n",
      "[0.9572243] [1. 0.]\n",
      "[0.86039203] [0. 1.]\n",
      "[0.70110816] [0. 1.]\n",
      "[0.2637984] [0. 1.]\n",
      "[0.14724705] [1. 0.]\n",
      "[0.98773015] [0. 1.]\n",
      "[0.16745271] [1. 0.]\n",
      "[0.97555125] [1. 0.]\n",
      "[0.98106885] [0. 1.]\n",
      "[0.14467202] [1. 0.]\n",
      "[0.9856134] [0. 1.]\n",
      "[0.95835316] [0. 1.]\n",
      "[0.18565713] [0. 1.]\n",
      "[0.98809046] [0. 1.]\n",
      "[0.15298939] [1. 0.]\n",
      "[0.12973829] [1. 0.]\n",
      "[0.16623636] [1. 0.]\n",
      "[0.98788416] [0. 1.]\n",
      "[0.98642415] [0. 1.]\n",
      "[0.14416045] [1. 0.]\n",
      "[0.23295265] [1. 0.]\n",
      "[0.21739946] [0. 1.]\n",
      "[0.15676978] [1. 0.]\n",
      "[0.9021872] [0. 1.]\n",
      "[0.9873407] [0. 1.]\n",
      "[0.95436114] [0. 1.]\n",
      "[0.26214492] [1. 0.]\n",
      "[0.97615397] [0. 1.]\n",
      "[0.96896505] [0. 1.]\n",
      "[0.14891899] [1. 0.]\n",
      "[0.17576562] [1. 0.]\n",
      "[0.9888393] [0. 1.]\n",
      "[0.9873353] [0. 1.]\n",
      "[0.14854632] [0. 1.]\n",
      "[0.9887948] [0. 1.]\n",
      "[0.9875839] [0. 1.]\n",
      "[0.98526067] [0. 1.]\n",
      "[0.9879755] [0. 1.]\n",
      "[0.7025647] [0. 1.]\n",
      "[0.14012209] [1. 0.]\n",
      "[0.9066861] [0. 1.]\n",
      "[0.61225706] [1. 0.]\n",
      "[0.6220827] [1. 0.]\n",
      "[0.2587176] [0. 1.]\n",
      "[0.9875515] [0. 1.]\n",
      "[0.21123855] [1. 0.]\n",
      "[0.14522547] [1. 0.]\n",
      "[0.88002324] [0. 1.]\n",
      "[0.36435878] [0. 1.]\n",
      "[0.93495786] [0. 1.]\n",
      "[0.98887795] [0. 1.]\n",
      "[0.7142704] [1. 0.]\n",
      "[0.9882413] [0. 1.]\n",
      "[0.7014361] [0. 1.]\n",
      "[0.19635053] [0. 1.]\n",
      "[0.986327] [0. 1.]\n",
      "[0.9870695] [0. 1.]\n",
      "[0.97641563] [0. 1.]\n",
      "[0.14846334] [1. 0.]\n",
      "[0.64399564] [0. 1.]\n",
      "[0.25402227] [1. 0.]\n",
      "[0.9886511] [0. 1.]\n",
      "[0.98621386] [0. 1.]\n",
      "[0.9874332] [0. 1.]\n",
      "[0.1598343] [1. 0.]\n",
      "[0.9820184] [0. 1.]\n",
      "[0.16407871] [1. 0.]\n",
      "[0.29135266] [0. 1.]\n",
      "[0.15034308] [1. 0.]\n",
      "[0.331403] [0. 1.]\n",
      "[0.95693] [0. 1.]\n",
      "[0.96538234] [0. 1.]\n",
      "[0.9854138] [0. 1.]\n",
      "[0.7378108] [0. 1.]\n",
      "[0.98889875] [0. 1.]\n",
      "[0.9823846] [0. 1.]\n",
      "[0.18513876] [0. 1.]\n",
      "[0.9855218] [0. 1.]\n",
      "[0.24997717] [1. 0.]\n",
      "[0.80980885] [1. 0.]\n",
      "[0.94713706] [0. 1.]\n",
      "[0.9660737] [1. 0.]\n",
      "[0.44196528] [0. 1.]\n",
      "[0.98792326] [0. 1.]\n",
      "[0.98844725] [0. 1.]\n",
      "[0.86921537] [1. 0.]\n",
      "[0.22057363] [1. 0.]\n",
      "[0.958766] [0. 1.]\n",
      "[0.9880635] [0. 1.]\n",
      "[0.15384342] [1. 0.]\n",
      "[0.12907335] [1. 0.]\n",
      "[0.98751295] [0. 1.]\n",
      "[0.9505009] [0. 1.]\n",
      "[0.9886506] [0. 1.]\n",
      "[0.76303947] [0. 1.]\n",
      "[0.17819959] [1. 0.]\n",
      "[0.25090247] [0. 1.]\n",
      "[0.16508603] [1. 0.]\n",
      "[0.9887391] [0. 1.]\n",
      "[0.98055464] [0. 1.]\n",
      "[0.98849154] [0. 1.]\n",
      "[0.13549435] [1. 0.]\n",
      "[0.9875562] [0. 1.]\n",
      "[0.6822103] [1. 0.]\n",
      "[0.11978944] [1. 0.]\n",
      "[0.98789597] [0. 1.]\n",
      "[0.9884357] [0. 1.]\n",
      "[0.8717772] [1. 0.]\n",
      "[0.98563254] [0. 1.]\n",
      "[0.17466895] [1. 0.]\n",
      "[0.16933396] [0. 1.]\n",
      "[0.9858471] [0. 1.]\n",
      "[0.1459815] [0. 1.]\n",
      "[0.9875742] [0. 1.]\n",
      "[0.16170481] [0. 1.]\n",
      "[0.74745566] [0. 1.]\n",
      "[0.18434443] [1. 0.]\n",
      "[0.7065117] [1. 0.]\n",
      "[0.14729945] [0. 1.]\n",
      "[0.18049856] [1. 0.]\n",
      "[0.15464796] [1. 0.]\n",
      "[0.9898864] [0. 1.]\n",
      "[0.18901312] [1. 0.]\n",
      "[0.9877352] [0. 1.]\n",
      "[0.1519706] [0. 1.]\n",
      "[0.8770866] [0. 1.]\n",
      "[0.16331661] [1. 0.]\n",
      "[0.6973316] [0. 1.]\n",
      "[0.9871111] [0. 1.]\n",
      "[0.13311823] [1. 0.]\n",
      "[0.9705766] [0. 1.]\n",
      "[0.53292876] [1. 0.]\n",
      "[0.97457707] [0. 1.]\n",
      "[0.9885346] [0. 1.]\n",
      "[0.97417027] [0. 1.]\n",
      "[0.5035229] [0. 1.]\n",
      "[0.21831197] [1. 0.]\n",
      "[0.15866436] [1. 0.]\n",
      "[0.97481805] [0. 1.]\n",
      "[0.9895391] [0. 1.]\n",
      "[0.97899795] [0. 1.]\n",
      "[0.9891229] [0. 1.]\n",
      "[0.31864354] [1. 0.]\n",
      "[0.98687214] [0. 1.]\n",
      "[0.16438268] [1. 0.]\n",
      "[0.50003546] [0. 1.]\n",
      "[0.8052113] [0. 1.]\n",
      "[0.9874425] [0. 1.]\n",
      "[0.9528763] [0. 1.]\n",
      "[0.97757435] [0. 1.]\n",
      "[0.8863706] [0. 1.]\n",
      "[0.97832453] [1. 0.]\n",
      "[0.4067789] [0. 1.]\n",
      "[0.13755596] [1. 0.]\n",
      "[0.13205756] [1. 0.]\n",
      "[0.98828757] [0. 1.]\n",
      "[0.73162585] [0. 1.]\n",
      "[0.7507808] [0. 1.]\n",
      "[0.98592573] [1. 0.]\n",
      "[0.8704834] [0. 1.]\n",
      "[0.9850418] [1. 0.]\n",
      "[0.7675842] [1. 0.]\n",
      "[0.2768169] [1. 0.]\n",
      "[0.9889043] [0. 1.]\n",
      "[0.98563004] [1. 0.]\n",
      "[0.98303515] [0. 1.]\n",
      "[0.8468913] [0. 1.]\n",
      "[0.98912406] [1. 0.]\n",
      "[0.13364269] [1. 0.]\n",
      "[0.88849634] [0. 1.]\n",
      "[0.9649551] [0. 1.]\n",
      "[0.98914254] [0. 1.]\n",
      "[0.9527291] [1. 0.]\n",
      "[0.7929693] [0. 1.]\n",
      "[0.987843] [0. 1.]\n",
      "[0.14762779] [1. 0.]\n",
      "[0.98539084] [0. 1.]\n",
      "[0.9876607] [0. 1.]\n",
      "[0.16555463] [1. 0.]\n",
      "[0.16099444] [1. 0.]\n",
      "[0.16261621] [1. 0.]\n",
      "[0.19227141] [0. 1.]\n",
      "[0.60718274] [0. 1.]\n",
      "[0.785399] [0. 1.]\n",
      "[0.6352694] [0. 1.]\n",
      "[0.9883738] [0. 1.]\n",
      "[0.1916355] [0. 1.]\n",
      "[0.7617755] [0. 1.]\n",
      "[0.5464072] [0. 1.]\n",
      "[0.15738887] [1. 0.]\n",
      "[0.9879387] [0. 1.]\n",
      "[0.161227] [1. 0.]\n",
      "[0.9870143] [0. 1.]\n",
      "[0.16196994] [1. 0.]\n",
      "[0.20964006] [1. 0.]\n",
      "[0.7988864] [0. 1.]\n",
      "[0.21562023] [0. 1.]\n",
      "[0.98707396] [0. 1.]\n",
      "[0.81589156] [0. 1.]\n",
      "[0.23596223] [1. 0.]\n",
      "[0.96044904] [0. 1.]\n",
      "[0.9889616] [0. 1.]\n",
      "[0.18090019] [1. 0.]\n",
      "[0.97418344] [1. 0.]\n",
      "[0.98590946] [0. 1.]\n",
      "[0.8099423] [0. 1.]\n",
      "[0.98686844] [0. 1.]\n",
      "[0.16673021] [1. 0.]\n",
      "[0.9861382] [0. 1.]\n",
      "[0.2038479] [1. 0.]\n",
      "[0.9806539] [0. 1.]\n",
      "[0.1710686] [1. 0.]\n",
      "[0.14304613] [1. 0.]\n",
      "[0.15786524] [1. 0.]\n",
      "[0.7599349] [0. 1.]\n",
      "[0.94941735] [0. 1.]\n",
      "[0.9880707] [0. 1.]\n",
      "[0.98807895] [0. 1.]\n",
      "[0.9863129] [0. 1.]\n",
      "[0.79011196] [0. 1.]\n",
      "[0.87092817] [0. 1.]\n",
      "[0.24445115] [0. 1.]\n",
      "[0.20999369] [0. 1.]\n",
      "[0.13629907] [1. 0.]\n",
      "[0.61303926] [0. 1.]\n",
      "[0.7109695] [1. 0.]\n",
      "[0.8600328] [0. 1.]\n",
      "[0.9875321] [0. 1.]\n",
      "[0.98863286] [0. 1.]\n",
      "[0.75298405] [1. 0.]\n",
      "[0.9890152] [0. 1.]\n",
      "[0.9864217] [0. 1.]\n",
      "[0.9555806] [0. 1.]\n",
      "[0.9883192] [0. 1.]\n",
      "[0.15855357] [1. 0.]\n",
      "[0.17181788] [1. 0.]\n",
      "[0.98917264] [0. 1.]\n",
      "[0.88993835] [0. 1.]\n",
      "[0.98885083] [0. 1.]\n",
      "[0.89019734] [0. 1.]\n",
      "[0.98635554] [1. 0.]\n",
      "[0.9858696] [0. 1.]\n",
      "[0.9897554] [0. 1.]\n",
      "[0.9880855] [0. 1.]\n",
      "[0.17415926] [1. 0.]\n",
      "[0.8896727] [0. 1.]\n",
      "[0.15788504] [1. 0.]\n",
      "[0.29720536] [0. 1.]\n",
      "[0.98812145] [0. 1.]\n",
      "[0.94961715] [0. 1.]\n",
      "[0.16334237] [1. 0.]\n",
      "[0.19359061] [0. 1.]\n",
      "[0.97623193] [0. 1.]\n",
      "[0.14506844] [1. 0.]\n",
      "[0.14712264] [1. 0.]\n",
      "[0.1695887] [1. 0.]\n",
      "[0.98666555] [0. 1.]\n",
      "[0.7056445] [0. 1.]\n",
      "[0.7737493] [0. 1.]\n",
      "[0.9736957] [0. 1.]\n",
      "[0.17325078] [1. 0.]\n",
      "[0.98829824] [0. 1.]\n",
      "[0.307004] [1. 0.]\n",
      "[0.20182624] [1. 0.]\n",
      "[0.3154812] [1. 0.]\n",
      "[0.7471156] [0. 1.]\n",
      "[0.86960447] [1. 0.]\n",
      "[0.9658898] [0. 1.]\n",
      "[0.89946806] [0. 1.]\n",
      "[0.96036744] [0. 1.]\n",
      "[0.9893385] [0. 1.]\n",
      "[0.98881495] [0. 1.]\n",
      "[0.13067764] [1. 0.]\n",
      "[0.98369503] [0. 1.]\n",
      "[0.64618206] [1. 0.]\n",
      "[0.98003316] [0. 1.]\n",
      "[0.19111496] [1. 0.]\n",
      "[0.16658497] [1. 0.]\n",
      "[0.9325253] [0. 1.]\n",
      "[0.91736203] [0. 1.]\n",
      "[0.77974236] [0. 1.]\n",
      "[0.1416126] [1. 0.]\n",
      "[0.9896213] [0. 1.]\n",
      "[0.8007844] [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(pred, y):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HANDY', 'AND', ':', 'YOU', 'CAN', 'USE', 'IT', 'ALMOST', 'ANYWHERE']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'HANDY AND : YOU CAN USE IT ALMOST ANYWHERE'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating matrices...\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 14:21:59.017363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-03 14:21:59.023846: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/peyman/anaconda3/envs/test2/lib/\n",
      "2022-12-03 14:21:59.023874: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-03 14:21:59.027038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings\n",
    "\n",
    "\n",
    "\n",
    "def create_y_matrix(y_data):\n",
    "  y_matrix = np.zeros((len(y_data),2))\n",
    "  for count,i in enumerate(y_data):\n",
    "    if i == 1:\n",
    "      y_matrix[count][1] = 1.0\n",
    "    else:\n",
    "      y_matrix[count][0] = 1.0\n",
    "  return y_matrix\n",
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        # print(line)\n",
    "        # print(i)\n",
    "        try:\n",
    "            words = line.split()\n",
    "            words = words[:batch_size] #cut off if too long\n",
    "            for j, word in enumerate(words):\n",
    "                if word in w2v:\n",
    "                    x_matrix[i, j, :] = w2v[word]\n",
    "        except:\n",
    "            pass\n",
    "    return x_matrix\n",
    "\n",
    "\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))              \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',Precision(),Recall(),AUC()] )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    eval = model.evaluate(X_test, y_test)\n",
    "    loss = eval[0]\n",
    "    accuracy = eval[1]\n",
    "    precision = eval[2]\n",
    "    recall = eval[3]\n",
    "    auc = eval[4]    \n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    # print(\"Loss: \", loss)\n",
    "    # print(\"Accuracy: \", accuracy)\n",
    "    # print(\"Precision: \", precision)\n",
    "    # print(\"Recall: \", recall)\n",
    "    # print(\"AUC: \", auc)\n",
    "    # print ('f1 score: ', f1_score)\n",
    "    return loss, accuracy, precision, recall, auc, f1_score\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "    # y_train_aug = to_categorical(y_train_aug)\n",
    "    # y_train_original = to_categorical(y_train_original)\n",
    "    # y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    print('Creating matrices...')\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    print('Training model...')\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=1024,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=1024,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    loss_org, accuracy_org, precision_org, recall_org, auc_org, f1_score_org = evaluate_model(model_original, X_test, y_test)\n",
    "    loss_aug, accuracy_aug, precision_aug, recall_aug, auc_aug, f1_score_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {accuracy_org:.4f} \\n f1: {f1_score_org:.4f} \\n precision: {precision_org:.4f} \\n recall: {recall_org:.4f}, \\n auc: {auc_org:.4f}')\n",
    "    print(f'augmented model: \\n acc: {accuracy_aug:.4f} \\n f1: {f1_score_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}, \\n auc: {auc_aug:.4f}')\n",
    "\n",
    "    print('finished')\n",
    "\n",
    "\n",
    "\n",
    "run('pc','aeda_augmenter',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import runai.ga.keras\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  #get rid of warnings\n",
    "\n",
    "\n",
    "\n",
    "def create_y_matrix(y_data):\n",
    "  y_matrix = np.zeros((len(y_data),2))\n",
    "  for count,i in enumerate(y_data):\n",
    "    if i == 1:\n",
    "      y_matrix[count][1] = 1.0\n",
    "    else:\n",
    "      y_matrix[count][0] = 1.0\n",
    "  return y_matrix\n",
    "\n",
    "def create_X_matrix(dataset, w2v,word2vec_len=300, batch_size=25):\n",
    "    dataset_size = len(dataset)\n",
    "    x_matrix = np.zeros((dataset_size, batch_size, word2vec_len))    \n",
    "    for i, line in enumerate(dataset):\n",
    "        try:\n",
    "            words = line.split()\n",
    "            words = words[:batch_size] #cut off if too long\n",
    "            for j, word in enumerate(words):\n",
    "                if word in w2v:\n",
    "                    x_matrix[i, j, :] = w2v[word]\n",
    "        except:\n",
    "            pass\n",
    "    return x_matrix\n",
    "\n",
    "\n",
    "def build_model(batch_size=25, word2vec_len=300):\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(batch_size, word2vec_len)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))  \n",
    "\n",
    "    STEPS = 10000\n",
    "    optimizer = runai.ga.keras.optimizers.Optimizer(Adam(), steps=STEPS)            \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc',Precision(),Recall(),AUC()] )\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    eval = model.evaluate(X_test, y_test)\n",
    "    loss = eval[0]\n",
    "    accuracy = eval[1]\n",
    "    precision = eval[2]\n",
    "    recall = eval[3]\n",
    "    auc = eval[4]    \n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    # print(\"Loss: \", loss)\n",
    "    # print(\"Accuracy: \", accuracy)\n",
    "    # print(\"Precision: \", precision)\n",
    "    # print(\"Recall: \", recall)\n",
    "    # print(\"AUC: \", auc)\n",
    "    # print ('f1 score: ', f1_score)\n",
    "    return loss, accuracy, precision, recall, auc, f1_score\n",
    "\n",
    "\n",
    "def run(dataset_name,aug_method,n_sample):\n",
    "    # Load data\n",
    "    print('Loading data...')\n",
    "    path_train_original = f'data/{dataset_name}/train.txt'\n",
    "    path_train_aug = f'data/{dataset_name}/train_{dataset_name}_{aug_method}_n_sample_{n_sample}.csv'\n",
    "    path_test = f'data/{dataset_name}/test.txt'\n",
    "    train_aug = load_data(path_train_aug)\n",
    "    train_original = load_data(path_train_original)\n",
    "    test_data = load_data(path_test)\n",
    "    X_train_aug, y_train_aug = train_aug['text'].values, train_aug['class'].values.astype(float)\n",
    "    X_train_original, y_train_original = train_original['text'].values, train_original['class'].values.astype(float)\n",
    "    X_test, y_test = test_data['text'].values, test_data['class'].values.astype(float)\n",
    "\n",
    "    # y_train_aug = to_categorical(y_train_aug)\n",
    "    # y_train_original = to_categorical(y_train_original)\n",
    "    # y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    # load wor2vec pickle\n",
    "    path_w2v = f'data/{dataset_name}/word2vec.p'\n",
    "    w2v = pickle.load(open(path_w2v, 'rb'))\n",
    "\n",
    "\n",
    "    # create matrices\n",
    "    print('Creating matrices...')\n",
    "    X_train_aug = create_X_matrix(X_train_aug, w2v)\n",
    "    X_train_original = create_X_matrix(X_train_original, w2v)\n",
    "    X_test = create_X_matrix(X_test, w2v)\n",
    "\n",
    "    # Train model\n",
    "    print('Training model...')\n",
    "    model_aug = build_model()\n",
    "    model_original = build_model()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    model_aug.fit(X_train_aug, y_train_aug, epochs=1000,\n",
    "                 callbacks=callbacks, validation_split=0.1,\n",
    "                  batch_size=128,shuffle=True, verbose=0)\n",
    "    model_original.fit(X_train_original, y_train_original, epochs=1000,\n",
    "                    callbacks=callbacks, validation_split=0.1,\n",
    "                    batch_size=128,shuffle=True, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print('Evaluating model...')\n",
    "\n",
    "    loss_org, accuracy_org, precision_org, recall_org, auc_org, f1_score_org = evaluate_model(model_original, X_test, y_test)\n",
    "    loss_aug, accuracy_aug, precision_aug, recall_aug, auc_aug, f1_score_aug = evaluate_model(model_aug, X_test, y_test)\n",
    "    print(f'original model: \\n acc: {accuracy_org:.4f} \\n f1: {f1_score_org:.4f} \\n precision: {precision_org:.4f} \\n recall: {recall_org:.4f}, \\n auc: {auc_org:.4f}')\n",
    "    print(f'augmented model: \\n acc: {accuracy_aug:.4f} \\n f1: {f1_score_aug:.4f} \\n precision: {precision_aug:.4f} \\n recall: {recall_aug:.4f}, \\n auc: {auc_aug:.4f}')\n",
    "\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating matrices...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.1 GiB for an array with shape (360000, 25, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/peyman/nlp_da_comparison/experiments/experiments.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run(\u001b[39m'\u001b[39;49m\u001b[39mpc\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39maeda_augmenter\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m8\u001b[39;49m)\n",
      "\u001b[1;32m/home/peyman/nlp_da_comparison/experiments/experiments.ipynb Cell 18\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset_name, aug_method, n_sample)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39m# create matrices\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCreating matrices...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m X_train_aug \u001b[39m=\u001b[39m create_X_matrix(X_train_aug, w2v)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m X_train_original \u001b[39m=\u001b[39m create_X_matrix(X_train_original, w2v)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m X_test \u001b[39m=\u001b[39m create_X_matrix(X_test, w2v)\n",
      "\u001b[1;32m/home/peyman/nlp_da_comparison/experiments/experiments.ipynb Cell 18\u001b[0m in \u001b[0;36mcreate_X_matrix\u001b[0;34m(dataset, w2v, word2vec_len, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_X_matrix\u001b[39m(dataset, w2v,word2vec_len\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     dataset_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataset)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     x_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((dataset_size, batch_size, word2vec_len))    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpc12740.mathematik.uni-marburg.de/home/peyman/nlp_da_comparison/experiments/experiments.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 20.1 GiB for an array with shape (360000, 25, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "run('pc','aeda_augmenter',8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024//100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba22cf2efef377717f89a84cea157a0c07bbb13bc3a02b99b621aabdf4befcc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

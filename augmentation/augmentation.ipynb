{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ../data/cardio/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 12:06:53.394646: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import \\\n",
    "    EasyDataAugmenter, BackTranslationAugmenter, WordNetAugmenter, CLAREAugmenter, \\\n",
    "    CheckListAugmenter, EmbeddingAugmenter, DeletionAugmenter, CharSwapAugmenter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "eda_augmenter = EasyDataAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "wordnet_augmenter = WordNetAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "clare_augmenter = CLAREAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "backtranslation_augmenter = BackTranslationAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "checklist_augmenter = CheckListAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "embedding_augmenter = EmbeddingAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "deletion_augmenter = DeletionAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "charswap_augmenter = CharSwapAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n",
    "list_of_augmenters = [eda_augmenter, wordnet_augmenter, clare_augmenter, backtranslation_augmenter, deletion_augmenter]\n",
    "\n",
    "#list_of_augmenters = [eda_augmenter, wordnet_augmenter, clare_augmenter, backtranslation_augmenter, checklist_augmenter, embedding_augmenter, deletion_augmenter, charswap_augmenter]\n",
    "sentence = 'What I cannot create, I do not understand.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyDataAugmenter\n",
      "['What I cannot create, ane I do not understand.', 'What I cannot create, I do understand.', 'What iodin cannot create, I do not understand.', 'What I cannot create, do I not understand.']\n",
      "WordNetAugmenter\n",
      "['What I cannot create, I do not realise.', 'What I cannot create, I do not realize.', 'What I cannot create, iodin do not understand.', 'What one cannot create, I do not understand.']\n",
      "CLAREAugmenter\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/notebooks/uni/nlp_da_comparison/augmentation/augmentation.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646576227d/home/notebooks/uni/nlp_da_comparison/augmentation/augmentation.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m augmenter \u001b[39min\u001b[39;00m list_of_augmenters:\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646576227d/home/notebooks/uni/nlp_da_comparison/augmentation/augmentation.ipynb#ch0000003vscode-remote?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(augmenter\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646576227d/home/notebooks/uni/nlp_da_comparison/augmentation/augmentation.ipynb#ch0000003vscode-remote?line=2'>3</a>\u001b[0m     res \u001b[39m=\u001b[39m augmenter\u001b[39m.\u001b[39;49maugment(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f646576227d/home/notebooks/uni/nlp_da_comparison/augmentation/augmentation.ipynb#ch0000003vscode-remote?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(res)\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/textattack/augmentation/augmenter.py:125\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    122\u001b[0m words_swapped \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(current_text\u001b[39m.\u001b[39mattack_attrs[\u001b[39m\"\u001b[39m\u001b[39mmodified_indices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    124\u001b[0m \u001b[39mwhile\u001b[39;00m words_swapped \u001b[39m<\u001b[39m num_words_to_swap:\n\u001b[0;32m--> 125\u001b[0m     transformed_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformation(\n\u001b[1;32m    126\u001b[0m         current_text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_transformation_constraints\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     \u001b[39m# Get rid of transformations we already have\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     transformed_texts \u001b[39m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m         t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m transformed_texts \u001b[39mif\u001b[39;00m t \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m all_transformed_texts\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/textattack/transformations/composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m new_attacked_texts \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m transformation \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformations:\n\u001b[0;32m---> 39\u001b[0m     new_attacked_texts\u001b[39m.\u001b[39mupdate(transformation(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/textattack/transformations/transformation.py:50\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[0;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m constraint \u001b[39min\u001b[39;00m pre_transformation_constraints:\n\u001b[1;32m     49\u001b[0m     indices_to_modify \u001b[39m=\u001b[39m indices_to_modify \u001b[39m&\u001b[39m constraint(current_text, \u001b[39mself\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m transformed_texts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_transformations(current_text, indices_to_modify)\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m transformed_texts:\n\u001b[1;32m     52\u001b[0m     text\u001b[39m.\u001b[39mattack_attrs[\u001b[39m\"\u001b[39m\u001b[39mlast_transformation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/textattack/transformations/word_swaps/word_swap_masked_lm.py:285\u001b[0m, in \u001b[0;36mWordSwapMaskedLM._get_transformations\u001b[0;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m transformed_texts\n\u001b[1;32m    284\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbae\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 285\u001b[0m     replacement_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bae_replacement_words(\n\u001b[1;32m    286\u001b[0m         current_text, indices_to_modify\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m     transformed_texts \u001b[39m=\u001b[39m []\n\u001b[1;32m    289\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(replacement_words)):\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/textattack/transformations/word_swaps/word_swap_masked_lm.py:130\u001b[0m, in \u001b[0;36mWordSwapMaskedLM._bae_replacement_words\u001b[0;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[1;32m    128\u001b[0m ids \u001b[39m=\u001b[39m inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    129\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 130\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_language_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(ids)):\n\u001b[1;32m    133\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m         \u001b[39m# Need try-except b/c mask-token located past max_length might be truncated by tokenizer\u001b[39;00m\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:971\u001b[0m, in \u001b[0;36mRobertaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 971\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m    972\u001b[0m     input_ids,\n\u001b[1;32m    973\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    974\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    975\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    976\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    977\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    978\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    979\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    980\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    981\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    982\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    983\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    984\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    985\u001b[0m )\n\u001b[1;32m    987\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    988\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:823\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    819\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    821\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m--> 823\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_extended_attention_mask(attention_mask, input_shape, device)\n\u001b[1;32m    825\u001b[0m \u001b[39m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_decoder \u001b[39mand\u001b[39;00m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.envs/env_da_project/lib/python3.8/site-packages/transformers/modeling_utils.py:301\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong shape for input_ids (shape \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m) or attention_mask (shape \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[39m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m extended_attention_mask\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype)  \u001b[39m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m    302\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m\n\u001b[1;32m    303\u001b[0m \u001b[39mreturn\u001b[39;00m extended_attention_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "for augmenter in list_of_augmenters:\n",
    "    print(augmenter.__class__.__name__)\n",
    "    res = augmenter.augment(sentence)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eda_augmenter.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import transformations, contraints, and the Augmenter\n",
    "# from textattack.transformations import WordSwapMaskedLM\n",
    "# from textattack.transformations import WordSwapQWERTY\n",
    "# from textattack.transformations import CompositeTransformation\n",
    "# from textattack.transformations import Transformation\n",
    "\n",
    "# from textattack.constraints.pre_transformation import RepeatModification\n",
    "# from textattack.constraints.pre_transformation import StopwordModification\n",
    "\n",
    "# from textattack.augmentation import Augmenter\n",
    "\n",
    "# # Set up transformation using CompositeTransformation()\n",
    "# transformation = CompositeTransformation([WordSwapMaskedLM()])\n",
    "# # Set up constraints\n",
    "# constraints = [RepeatModification(), StopwordModification()]\n",
    "# # Create augmenter with specified parameters\n",
    "# augmenter = Augmenter(transformation=transformation, constraints=constraints, pct_words_to_swap=0.2, transformations_per_example=10)\n",
    "# s = 'What I cannot create, I do not understand.'\n",
    "# # Augment!\n",
    "# augmenter.augment(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clare_augmenter = CLAREAugmenter(pct_words_to_swap=0.2,transformations_per_example=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'What I cannot create, I do not understand.'\n",
    "clare_augmenter.augment(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_da_project",
   "language": "python",
   "name": "env_da_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "119a70d207c1d1f8f9e0381d777f0ca81cc7f4a189ffdee6c2bd560444dcdca1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
